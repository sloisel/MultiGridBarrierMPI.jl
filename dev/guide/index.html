<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>User Guide · MultiGridBarrierMPI.jl 0.1.1</title><meta name="title" content="User Guide · MultiGridBarrierMPI.jl 0.1.1"/><meta property="og:title" content="User Guide · MultiGridBarrierMPI.jl 0.1.1"/><meta property="twitter:title" content="User Guide · MultiGridBarrierMPI.jl 0.1.1"/><meta name="description" content="Documentation for MultiGridBarrierMPI.jl 0.1.1."/><meta property="og:description" content="Documentation for MultiGridBarrierMPI.jl 0.1.1."/><meta property="twitter:description" content="Documentation for MultiGridBarrierMPI.jl 0.1.1."/><meta property="og:url" content="https://sloisel.github.io/MultiGridBarrierMPI.jl/guide/"/><meta property="twitter:url" content="https://sloisel.github.io/MultiGridBarrierMPI.jl/guide/"/><link rel="canonical" href="https://sloisel.github.io/MultiGridBarrierMPI.jl/guide/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultiGridBarrierMPI.jl 0.1.1</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li class="is-active"><a class="tocitem" href>User Guide</a><ul class="internal"><li><a class="tocitem" href="#Initialization"><span>Initialization</span></a></li><li><a class="tocitem" href="#Basic-Workflow"><span>Basic Workflow</span></a></li><li><a class="tocitem" href="#Understanding-MPI-Collective-Operations"><span>Understanding MPI Collective Operations</span></a></li><li><a class="tocitem" href="#Type-Conversions"><span>Type Conversions</span></a></li><li><a class="tocitem" href="#Advanced-Usage"><span>Advanced Usage</span></a></li><li><a class="tocitem" href="#IO-and-Output"><span>IO and Output</span></a></li><li><a class="tocitem" href="#Performance-Considerations"><span>Performance Considerations</span></a></li><li><a class="tocitem" href="#1D-Problems"><span>1D Problems</span></a></li><li><a class="tocitem" href="#2D-Problems"><span>2D Problems</span></a></li><li><a class="tocitem" href="#3D-Problems"><span>3D Problems</span></a></li><li><a class="tocitem" href="#Time-Dependent-(Parabolic)-Problems"><span>Time-Dependent (Parabolic) Problems</span></a></li><li><a class="tocitem" href="#Common-Patterns"><span>Common Patterns</span></a></li><li><a class="tocitem" href="#Next-Steps"><span>Next Steps</span></a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>User Guide</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>User Guide</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sloisel/MultiGridBarrierMPI.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/main/docs/src/guide.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="User-Guide"><a class="docs-heading-anchor" href="#User-Guide">User Guide</a><a id="User-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#User-Guide" title="Permalink"></a></h1><p>This guide covers the essential workflows for using MultiGridBarrierMPI.jl.</p><h2 id="Initialization"><a class="docs-heading-anchor" href="#Initialization">Initialization</a><a id="Initialization-1"></a><a class="docs-heading-anchor-permalink" href="#Initialization" title="Permalink"></a></h2><p>Every program using MultiGridBarrierMPI.jl must initialize MPI before using the package:</p><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI</code></pre><h2 id="Basic-Workflow"><a class="docs-heading-anchor" href="#Basic-Workflow">Basic Workflow</a><a id="Basic-Workflow-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Workflow" title="Permalink"></a></h2><p>The typical workflow consists of three steps:</p><ol><li><strong>Solve with MPI types</strong> (distributed computation)</li><li><strong>Convert to native types</strong> (for analysis/plotting)</li><li><strong>Visualize or analyze</strong> (using MultiGridBarrier&#39;s tools)</li></ol><h3 id="Complete-Example-with-Visualization"><a class="docs-heading-anchor" href="#Complete-Example-with-Visualization">Complete Example with Visualization</a><a id="Complete-Example-with-Visualization-1"></a><a class="docs-heading-anchor-permalink" href="#Complete-Example-with-Visualization" title="Permalink"></a></h3><p>Here&#39;s a complete example that solves a 2D FEM problem, converts the solution, and plots it:</p><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI
using LinearAlgebraMPI
using MultiGridBarrier

# Step 1: Solve with MPI distributed types
sol_mpi = fem2d_mpi_solve(Float64; L=3, p=1.0, verbose=false)

# Step 2: Convert solution to native Julia types
sol_native = mpi_to_native(sol_mpi)

# Step 3: Plot the solution using MultiGridBarrier&#39;s plot function
rank = MPI.Comm_rank(MPI.COMM_WORLD)
if rank == 0
    using PyPlot
    figure(figsize=(10, 8))
    plot(sol_native)
    title(&quot;Multigrid Barrier Solution (L=3)&quot;)
    tight_layout()
    savefig(&quot;solution_plot.png&quot;)
end
println(io0(), &quot;Solution plotted!&quot;)</code></pre><div class="admonition is-success" id="Running-This-Example-85e0bdd162d3d7d"><header class="admonition-header">Running This Example<a class="admonition-anchor" href="#Running-This-Example-85e0bdd162d3d7d" title="Permalink"></a></header><div class="admonition-body"><p>Save this code to a file (e.g., <code>visualize.jl</code>) and run with:</p><pre><code class="language-bash hljs">mpiexec -n 4 julia --project visualize.jl</code></pre></div></div><h2 id="Understanding-MPI-Collective-Operations"><a class="docs-heading-anchor" href="#Understanding-MPI-Collective-Operations">Understanding MPI Collective Operations</a><a id="Understanding-MPI-Collective-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-MPI-Collective-Operations" title="Permalink"></a></h2><div class="admonition is-warning" id="All-Functions-Are-Collective-2396b8835fbbbab5"><header class="admonition-header">All Functions Are Collective<a class="admonition-anchor" href="#All-Functions-Are-Collective-2396b8835fbbbab5" title="Permalink"></a></header><div class="admonition-body"><p>All exported functions in MultiGridBarrierMPI.jl are <strong>MPI collective operations</strong>. This means:</p><ul><li>All MPI ranks must call the function</li><li>All ranks must call it with the same parameters</li><li>Deadlock will occur if only some ranks call a collective function</li></ul></div></div><p><strong>Correct usage:</strong></p><pre><code class="language-julia hljs"># All ranks execute this together
sol = fem2d_mpi_solve(Float64; L=2, p=1.0)</code></pre><p><strong>Incorrect usage (causes deadlock):</strong></p><pre><code class="language-julia hljs">rank = MPI.Comm_rank(MPI.COMM_WORLD)
if rank == 0
    sol = fem2d_mpi_solve(Float64; L=2, p=1.0)  # Only rank 0 calls - DEADLOCK!
end</code></pre><h2 id="Type-Conversions"><a class="docs-heading-anchor" href="#Type-Conversions">Type Conversions</a><a id="Type-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#Type-Conversions" title="Permalink"></a></h2><h3 id="Native-to-MPI"><a class="docs-heading-anchor" href="#Native-to-MPI">Native to MPI</a><a id="Native-to-MPI-1"></a><a class="docs-heading-anchor-permalink" href="#Native-to-MPI" title="Permalink"></a></h3><p>Convert native Julia arrays to MPI distributed types:</p><pre><code class="language-julia hljs">using MultiGridBarrier

# Create native geometry
g_native = fem2d(; L=2)

# Convert to MPI types for distributed computation
g_mpi = native_to_mpi(g_native)</code></pre><p><strong>Type mappings:</strong></p><table><tr><th style="text-align: right">Native Type</th><th style="text-align: right">MPI Type</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>Matrix{T}</code></td><td style="text-align: right"><code>MatrixMPI{T}</code></td><td style="text-align: right">Dense distributed matrix</td></tr><tr><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right"><code>VectorMPI{T}</code></td><td style="text-align: right">Dense distributed vector</td></tr><tr><td style="text-align: right"><code>SparseMatrixCSC{T,Int}</code></td><td style="text-align: right"><code>SparseMatrixMPI{T}</code></td><td style="text-align: right">Sparse distributed matrix</td></tr></table><h3 id="MPI-to-Native"><a class="docs-heading-anchor" href="#MPI-to-Native">MPI to Native</a><a id="MPI-to-Native-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-to-Native" title="Permalink"></a></h3><p>Convert MPI types back to native Julia arrays:</p><pre><code class="language-julia hljs"># Create and solve with MPI types
g_mpi = fem2d_mpi(Float64; L=2)
sol_mpi = amgb(g_mpi; p=2.0)

# Convert back for analysis
g_native = mpi_to_native(g_mpi)
sol_native = mpi_to_native(sol_mpi)

# Now you can use native Julia operations
using LinearAlgebra
z_matrix = sol_native.z
solution_norm = norm(z_matrix)
println(io0(), &quot;Solution norm: &quot;, solution_norm)</code></pre><h2 id="Advanced-Usage"><a class="docs-heading-anchor" href="#Advanced-Usage">Advanced Usage</a><a id="Advanced-Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Usage" title="Permalink"></a></h2><h3 id="Custom-Geometry-Workflow"><a class="docs-heading-anchor" href="#Custom-Geometry-Workflow">Custom Geometry Workflow</a><a id="Custom-Geometry-Workflow-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-Geometry-Workflow" title="Permalink"></a></h3><p>For more control, construct geometries manually:</p><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI
using LinearAlgebraMPI
using MultiGridBarrier

# 1. Create native geometry with specific parameters
g_native = fem2d(; L=2)

# 2. Convert to MPI for distributed solving
g_mpi = native_to_mpi(g_native)

# 3. Solve with custom barrier parameters
sol_mpi = amgb(g_mpi;
    p=1.5,           # Barrier power parameter
    verbose=true,    # Print convergence info
    maxit=100,       # Maximum iterations
    tol=1e-8)        # Convergence tolerance

# 4. Convert solution back
sol_native = mpi_to_native(sol_mpi)

# 5. Access solution components
println(io0(), &quot;Newton steps: &quot;, sum(sol_native.SOL_main.its))
println(io0(), &quot;Elapsed time: &quot;, sol_native.SOL_main.t_elapsed, &quot; seconds&quot;)</code></pre><h3 id="Comparing-MPI-vs-Native-Solutions"><a class="docs-heading-anchor" href="#Comparing-MPI-vs-Native-Solutions">Comparing MPI vs Native Solutions</a><a id="Comparing-MPI-vs-Native-Solutions-1"></a><a class="docs-heading-anchor-permalink" href="#Comparing-MPI-vs-Native-Solutions" title="Permalink"></a></h3><p>Verify that MPI and native implementations give the same results:</p><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI
using LinearAlgebraMPI
using MultiGridBarrier
using LinearAlgebra

# Solve with MPI (distributed)
sol_mpi_dist = fem2d_mpi_solve(Float64; L=2, p=1.0, verbose=false)
z_mpi = mpi_to_native(sol_mpi_dist).z

# Solve with native (sequential, on rank 0)
rank = MPI.Comm_rank(MPI.COMM_WORLD)
if rank == 0
    sol_native = MultiGridBarrier.fem2d_solve(Float64; L=2, p=1.0, verbose=false)
    z_native = sol_native.z

    # Compare solutions
    diff = norm(z_mpi - z_native) / norm(z_native)
    println(&quot;Relative difference: &quot;, diff)
    @assert diff &lt; 1e-10 &quot;Solutions should match!&quot;
end</code></pre><h2 id="IO-and-Output"><a class="docs-heading-anchor" href="#IO-and-Output">IO and Output</a><a id="IO-and-Output-1"></a><a class="docs-heading-anchor-permalink" href="#IO-and-Output" title="Permalink"></a></h2><h3 id="Printing-from-One-Rank"><a class="docs-heading-anchor" href="#Printing-from-One-Rank">Printing from One Rank</a><a id="Printing-from-One-Rank-1"></a><a class="docs-heading-anchor-permalink" href="#Printing-from-One-Rank" title="Permalink"></a></h3><p>Use <code>io0()</code> from LinearAlgebraMPI to print from rank 0 only:</p><pre><code class="language-julia hljs">using LinearAlgebraMPI

# This prints once (from rank 0)
println(io0(), &quot;Hello from rank 0!&quot;)

# Without io0(), this prints from ALL ranks
println(&quot;Hello from rank &quot;, MPI.Comm_rank(MPI.COMM_WORLD))</code></pre><h3 id="MPI-Rank-Information"><a class="docs-heading-anchor" href="#MPI-Rank-Information">MPI Rank Information</a><a id="MPI-Rank-Information-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-Rank-Information" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI

rank = MPI.Comm_rank(MPI.COMM_WORLD)  # Current rank (0 to nranks-1)
nranks = MPI.Comm_size(MPI.COMM_WORLD)  # Total number of ranks</code></pre><h2 id="Performance-Considerations"><a class="docs-heading-anchor" href="#Performance-Considerations">Performance Considerations</a><a id="Performance-Considerations-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-Considerations" title="Permalink"></a></h2><h3 id="Threading"><a class="docs-heading-anchor" href="#Threading">Threading</a><a id="Threading-1"></a><a class="docs-heading-anchor-permalink" href="#Threading" title="Permalink"></a></h3><p>For optimal performance, use BLAS threading with a single Julia thread:</p><pre><code class="language-bash hljs">export OMP_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=10  # or your number of CPU cores
mpiexec -n 1 julia --project my_program.jl</code></pre><ul><li><strong>BLAS threads</strong> (<code>OPENBLAS_NUM_THREADS</code>) - Controls threading for dense matrix operations in both Julia and MUMPS</li><li><strong>OpenMP threads</strong> (<code>OMP_NUM_THREADS</code>) - Should be set to 1; MUMPS uses BLAS threading internally</li></ul><p>You can also set these in Julia&#39;s startup.jl:</p><pre><code class="language-julia hljs"># In ~/.julia/config/startup.jl
ENV[&quot;OMP_NUM_THREADS&quot;] = &quot;1&quot;
ENV[&quot;OPENBLAS_NUM_THREADS&quot;] = string(Sys.CPU_THREADS)</code></pre><h3 id="Performance-Comparison-(Single-Rank)"><a class="docs-heading-anchor" href="#Performance-Comparison-(Single-Rank)">Performance Comparison (Single-Rank)</a><a id="Performance-Comparison-(Single-Rank)-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-Comparison-(Single-Rank)" title="Permalink"></a></h3><p>The following table compares MultiGridBarrierMPI (using MUMPS) against MultiGridBarrier.jl&#39;s native solver on a 2D p-Laplace problem. Both use <code>OMP_NUM_THREADS=1</code> and <code>OPENBLAS_NUM_THREADS=10</code>. Benchmarks were run on a 2025 M4 MacBook Pro with 10 CPU cores:</p><table><tr><th style="text-align: right">L</th><th style="text-align: right">n (grid points)</th><th style="text-align: right">Native (s)</th><th style="text-align: right">MPI (s)</th><th style="text-align: right">Ratio</th><th style="text-align: right">Diff</th></tr><tr><td style="text-align: right">1</td><td style="text-align: right">14</td><td style="text-align: right">0.018</td><td style="text-align: right">0.029</td><td style="text-align: right">1.60x</td><td style="text-align: right">0.00e+00</td></tr><tr><td style="text-align: right">2</td><td style="text-align: right">56</td><td style="text-align: right">0.028</td><td style="text-align: right">0.039</td><td style="text-align: right">1.41x</td><td style="text-align: right">8.88e-16</td></tr><tr><td style="text-align: right">3</td><td style="text-align: right">224</td><td style="text-align: right">0.074</td><td style="text-align: right">0.078</td><td style="text-align: right">1.06x</td><td style="text-align: right">4.13e-11</td></tr><tr><td style="text-align: right">4</td><td style="text-align: right">896</td><td style="text-align: right">0.480</td><td style="text-align: right">0.410</td><td style="text-align: right">0.85x</td><td style="text-align: right">1.04e-14</td></tr><tr><td style="text-align: right">5</td><td style="text-align: right">3,584</td><td style="text-align: right">2.508</td><td style="text-align: right">1.771</td><td style="text-align: right">0.71x</td><td style="text-align: right">3.55e-12</td></tr><tr><td style="text-align: right">6</td><td style="text-align: right">14,336</td><td style="text-align: right">26.384</td><td style="text-align: right">68.846</td><td style="text-align: right">2.61x</td><td style="text-align: right">1.08e-13</td></tr><tr><td style="text-align: right">7</td><td style="text-align: right">57,344</td><td style="text-align: right">96.694</td><td style="text-align: right">118.070</td><td style="text-align: right">1.22x</td><td style="text-align: right">3.31e-13</td></tr><tr><td style="text-align: right">8</td><td style="text-align: right">229,376</td><td style="text-align: right">659.276</td><td style="text-align: right">504.672</td><td style="text-align: right">0.77x</td><td style="text-align: right">1.31e-11</td></tr></table><p><em>Ratio = MPI time / Native time (lower is better, &lt;1.0 means MPI is faster)</em> <em>Diff = sup-norm difference between native and MPI solutions</em></p><p>For medium-sized problems (L=4-5) and large problems (L=8), the MPI version is faster than the native solver.</p><h2 id="1D-Problems"><a class="docs-heading-anchor" href="#1D-Problems">1D Problems</a><a id="1D-Problems-1"></a><a class="docs-heading-anchor-permalink" href="#1D-Problems" title="Permalink"></a></h2><p>MultiGridBarrierMPI supports 1D finite element problems.</p><h3 id="Basic-1D-Example"><a class="docs-heading-anchor" href="#Basic-1D-Example">Basic 1D Example</a><a id="Basic-1D-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-1D-Example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI
using LinearAlgebraMPI

# Solve a 1D problem with 4 multigrid levels (2^4 = 16 elements)
sol = fem1d_mpi_solve(Float64; L=4, p=1.0, verbose=true)

# Convert solution to native types for analysis
sol_native = mpi_to_native(sol)

println(io0(), &quot;Solution computed successfully!&quot;)
println(io0(), &quot;Newton steps: &quot;, sum(sol_native.SOL_main.its))</code></pre><h3 id="1D-Parameters"><a class="docs-heading-anchor" href="#1D-Parameters">1D Parameters</a><a id="1D-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#1D-Parameters" title="Permalink"></a></h3><p>The <code>fem1d_mpi</code> and <code>fem1d_mpi_solve</code> functions accept:</p><table><tr><th style="text-align: right">Parameter</th><th style="text-align: right">Description</th><th style="text-align: right">Default</th></tr><tr><td style="text-align: right"><code>L</code></td><td style="text-align: right">Number of multigrid levels (creates 2^L elements)</td><td style="text-align: right">4</td></tr></table><h2 id="2D-Problems"><a class="docs-heading-anchor" href="#2D-Problems">2D Problems</a><a id="2D-Problems-1"></a><a class="docs-heading-anchor-permalink" href="#2D-Problems" title="Permalink"></a></h2><h3 id="Basic-2D-Example"><a class="docs-heading-anchor" href="#Basic-2D-Example">Basic 2D Example</a><a id="Basic-2D-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-2D-Example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI
using LinearAlgebraMPI

# Solve a 2D problem
sol = fem2d_mpi_solve(Float64; L=2, p=1.0, verbose=true)

# Convert solution to native types for analysis
sol_native = mpi_to_native(sol)

println(io0(), &quot;Solution computed successfully!&quot;)
println(io0(), &quot;Newton steps: &quot;, sum(sol_native.SOL_main.its))</code></pre><h3 id="2D-Parameters"><a class="docs-heading-anchor" href="#2D-Parameters">2D Parameters</a><a id="2D-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#2D-Parameters" title="Permalink"></a></h3><table><tr><th style="text-align: right">Parameter</th><th style="text-align: right">Description</th><th style="text-align: right">Default</th></tr><tr><td style="text-align: right"><code>L</code></td><td style="text-align: right">Number of multigrid levels</td><td style="text-align: right">2</td></tr><tr><td style="text-align: right"><code>K</code></td><td style="text-align: right">Coarse mesh vertices (3n×2 matrix)</td><td style="text-align: right">Unit square triangulation</td></tr></table><h2 id="3D-Problems"><a class="docs-heading-anchor" href="#3D-Problems">3D Problems</a><a id="3D-Problems-1"></a><a class="docs-heading-anchor-permalink" href="#3D-Problems" title="Permalink"></a></h2><p>MultiGridBarrierMPI supports 3D hexahedral finite elements.</p><h3 id="Basic-3D-Example"><a class="docs-heading-anchor" href="#Basic-3D-Example">Basic 3D Example</a><a id="Basic-3D-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-3D-Example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI
using LinearAlgebraMPI

# Solve a 3D problem with Q3 elements and 2 multigrid levels
sol = fem3d_mpi_solve(Float64; L=2, k=3, p=1.0, verbose=true)

# Convert solution to native types for analysis
sol_native = mpi_to_native(sol)

println(io0(), &quot;Solution computed successfully!&quot;)
println(io0(), &quot;Newton steps: &quot;, sum(sol_native.SOL_main.its))</code></pre><h3 id="3D-Parameters"><a class="docs-heading-anchor" href="#3D-Parameters">3D Parameters</a><a id="3D-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#3D-Parameters" title="Permalink"></a></h3><table><tr><th style="text-align: right">Parameter</th><th style="text-align: right">Description</th><th style="text-align: right">Default</th></tr><tr><td style="text-align: right"><code>L</code></td><td style="text-align: right">Number of multigrid levels</td><td style="text-align: right">2</td></tr><tr><td style="text-align: right"><code>k</code></td><td style="text-align: right">Polynomial order of elements (Q_k)</td><td style="text-align: right">3</td></tr></table><h2 id="Time-Dependent-(Parabolic)-Problems"><a class="docs-heading-anchor" href="#Time-Dependent-(Parabolic)-Problems">Time-Dependent (Parabolic) Problems</a><a id="Time-Dependent-(Parabolic)-Problems-1"></a><a class="docs-heading-anchor-permalink" href="#Time-Dependent-(Parabolic)-Problems" title="Permalink"></a></h2><p>MultiGridBarrierMPI supports time-dependent parabolic PDEs through MultiGridBarrier.jl&#39;s <code>parabolic_solve</code> function.</p><h3 id="Basic-Parabolic-Example"><a class="docs-heading-anchor" href="#Basic-Parabolic-Example">Basic Parabolic Example</a><a id="Basic-Parabolic-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Parabolic-Example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI
using LinearAlgebraMPI
using MultiGridBarrier

# Create MPI geometry
g = fem2d_mpi(Float64; L=2)

# Solve time-dependent problem from t=0 to t=1 with timestep h=0.2
sol = parabolic_solve(g; h=0.2, p=1.0, verbose=true)

println(io0(), &quot;Parabolic solve completed!&quot;)
println(io0(), &quot;Number of timesteps: &quot;, length(sol.ts))</code></pre><h3 id="Converting-Parabolic-Solutions-to-Native-Types"><a class="docs-heading-anchor" href="#Converting-Parabolic-Solutions-to-Native-Types">Converting Parabolic Solutions to Native Types</a><a id="Converting-Parabolic-Solutions-to-Native-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Converting-Parabolic-Solutions-to-Native-Types" title="Permalink"></a></h3><pre><code class="language-julia hljs">g = fem2d_mpi(Float64; L=2)
sol_mpi = parabolic_solve(g; h=0.25, p=1.0, verbose=false)

# Convert to native types
sol_native = mpi_to_native(sol_mpi)

# Now sol_native.u contains Vector{Matrix{Float64}}
println(io0(), &quot;Native u type: &quot;, typeof(sol_native.u))
println(io0(), &quot;Snapshot size: &quot;, size(sol_native.u[1]))</code></pre><h2 id="Common-Patterns"><a class="docs-heading-anchor" href="#Common-Patterns">Common Patterns</a><a id="Common-Patterns-1"></a><a class="docs-heading-anchor-permalink" href="#Common-Patterns" title="Permalink"></a></h2><h3 id="Solve-and-Extract-Specific-Values"><a class="docs-heading-anchor" href="#Solve-and-Extract-Specific-Values">Solve and Extract Specific Values</a><a id="Solve-and-Extract-Specific-Values-1"></a><a class="docs-heading-anchor-permalink" href="#Solve-and-Extract-Specific-Values" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI
using LinearAlgebraMPI

sol = fem2d_mpi_solve(Float64; L=3, p=1.0)
sol_native = mpi_to_native(sol)

# Access solution data
z = sol_native.z  # Solution matrix
iters = sum(sol_native.SOL_main.its)  # Total Newton steps
elapsed = sol_native.SOL_main.t_elapsed  # Elapsed time in seconds

println(io0(), &quot;Converged in $iters iterations&quot;)
println(io0(), &quot;Elapsed time: $elapsed seconds&quot;)</code></pre><h2 id="Next-Steps"><a class="docs-heading-anchor" href="#Next-Steps">Next Steps</a><a id="Next-Steps-1"></a><a class="docs-heading-anchor-permalink" href="#Next-Steps" title="Permalink"></a></h2><ul><li>See the <a href="../api/#API-Reference">API Reference</a> for detailed function documentation</li><li>Check the <code>examples/</code> directory for complete runnable examples</li><li>Consult MultiGridBarrier.jl documentation for barrier method theory</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../installation/">« Installation</a><a class="docs-footer-nextpage" href="../api/">API Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Tuesday 6 January 2026 16:20">Tuesday 6 January 2026</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
