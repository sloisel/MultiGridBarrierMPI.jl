<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · MultiGridBarrierMPI.jl 0.1.1</title><meta name="title" content="API Reference · MultiGridBarrierMPI.jl 0.1.1"/><meta property="og:title" content="API Reference · MultiGridBarrierMPI.jl 0.1.1"/><meta property="twitter:title" content="API Reference · MultiGridBarrierMPI.jl 0.1.1"/><meta name="description" content="Documentation for MultiGridBarrierMPI.jl 0.1.1."/><meta property="og:description" content="Documentation for MultiGridBarrierMPI.jl 0.1.1."/><meta property="twitter:description" content="Documentation for MultiGridBarrierMPI.jl 0.1.1."/><meta property="og:url" content="https://sloisel.github.io/MultiGridBarrierMPI.jl/api/"/><meta property="twitter:url" content="https://sloisel.github.io/MultiGridBarrierMPI.jl/api/"/><link rel="canonical" href="https://sloisel.github.io/MultiGridBarrierMPI.jl/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MultiGridBarrierMPI.jl 0.1.1</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../guide/">User Guide</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#High-Level-API"><span>High-Level API</span></a></li><li><a class="tocitem" href="#Type-Conversion-API"><span>Type Conversion API</span></a></li><li><a class="tocitem" href="#Type-Mappings-Reference"><span>Type Mappings Reference</span></a></li><li><a class="tocitem" href="#Geometry-Structure"><span>Geometry Structure</span></a></li><li><a class="tocitem" href="#Solution-Structure"><span>Solution Structure</span></a></li><li><a class="tocitem" href="#MPI-and-IO-Utilities"><span>MPI and IO Utilities</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li><li><a class="tocitem" href="#Integration-with-MultiGridBarrier"><span>Integration with MultiGridBarrier</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sloisel/MultiGridBarrierMPI.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/main/docs/src/api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><p>This page provides detailed documentation for all exported functions in MultiGridBarrierMPI.jl.</p><div class="admonition is-info" id="All-Functions-Are-Collective-26ad599f38f0a84e"><header class="admonition-header">All Functions Are Collective<a class="admonition-anchor" href="#All-Functions-Are-Collective-26ad599f38f0a84e" title="Permalink"></a></header><div class="admonition-body"><p>All functions documented here are <strong>MPI collective operations</strong>. Every MPI rank must call these functions together with the same parameters. Failure to do so will result in deadlock.</p></div></div><h2 id="High-Level-API"><a class="docs-heading-anchor" href="#High-Level-API">High-Level API</a><a id="High-Level-API-1"></a><a class="docs-heading-anchor-permalink" href="#High-Level-API" title="Permalink"></a></h2><p>These functions provide the simplest interface for solving problems with MPI types.</p><h3 id="1D-Problems"><a class="docs-heading-anchor" href="#1D-Problems">1D Problems</a><a id="1D-Problems-1"></a><a class="docs-heading-anchor-permalink" href="#1D-Problems" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="MultiGridBarrierMPI.fem1d_mpi"><a class="docstring-binding" href="#MultiGridBarrierMPI.fem1d_mpi"><code>MultiGridBarrierMPI.fem1d_mpi</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">fem1d_mpi(::Type{T}=Float64; kwargs...) where {T}</code></pre><p><strong>Collective</strong></p><p>Create an MPI-based Geometry from fem1d parameters.</p><p>This function calls <code>fem1d(kwargs...)</code> to create a native 1D geometry, then converts it to use MPI distributed types for distributed computing.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>: Element type for the geometry (default: Float64)</li><li><code>kwargs...</code>: Additional keyword arguments passed to <code>fem1d()</code>:<ul><li><code>L::Int</code>: Number of multigrid levels (default: 4), creating 2^L elements</li></ul></li></ul><p><strong>Returns</strong></p><p>A Geometry object with MPI distributed types.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using MPI; MPI.Init()
using MultiGridBarrierMPI
g = fem1d_mpi(Float64; L=4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L393-L417">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MultiGridBarrierMPI.fem1d_mpi_solve"><a class="docstring-binding" href="#MultiGridBarrierMPI.fem1d_mpi_solve"><code>MultiGridBarrierMPI.fem1d_mpi_solve</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">fem1d_mpi_solve(::Type{T}=Float64; kwargs...) where {T}</code></pre><p><strong>Collective</strong></p><p>Solve a fem1d problem using amgb with MPI distributed types.</p><p>This is a convenience function that combines <code>fem1d_mpi</code> and <code>amgb</code> into a single call. It creates an MPI-based 1D geometry and solves the barrier problem.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>: Element type for the geometry (default: Float64)</li><li><code>kwargs...</code>: Keyword arguments passed to both <code>fem1d_mpi</code> and <code>amgb</code><ul><li><code>L::Int</code>: Number of multigrid levels (passed to fem1d)</li><li><code>p</code>: Power parameter for the barrier (passed to amgb)</li><li><code>verbose</code>: Verbosity flag (passed to amgb)</li><li>Other arguments specific to fem1d or amgb</li></ul></li></ul><p><strong>Returns</strong></p><p>The solution object from <code>amgb</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">sol = fem1d_mpi_solve(Float64; L=4, p=1.0, verbose=true)
println(&quot;Solution norm: &quot;, norm(sol.z))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L426-L452">source</a></section></details></article><h3 id="2D-Problems"><a class="docs-heading-anchor" href="#2D-Problems">2D Problems</a><a id="2D-Problems-1"></a><a class="docs-heading-anchor-permalink" href="#2D-Problems" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="MultiGridBarrierMPI.fem2d_mpi"><a class="docstring-binding" href="#MultiGridBarrierMPI.fem2d_mpi"><code>MultiGridBarrierMPI.fem2d_mpi</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">fem2d_mpi(::Type{T}=Float64; kwargs...) where {T}</code></pre><p><strong>Collective</strong></p><p>Create an MPI-based Geometry from fem2d parameters.</p><p>This function calls <code>fem2d(kwargs...)</code> to create a native geometry, then converts it to use MPI distributed types for distributed computing.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>: Element type for the geometry (default: Float64)</li><li><code>kwargs...</code>: Additional keyword arguments passed to <code>fem2d()</code></li></ul><p><strong>Returns</strong></p><p>A Geometry object with MPI distributed types.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using MPI; MPI.Init()
using MultiGridBarrierMPI
g = fem2d_mpi(Float64; maxh=0.1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L461-L484">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MultiGridBarrierMPI.fem2d_mpi_solve"><a class="docstring-binding" href="#MultiGridBarrierMPI.fem2d_mpi_solve"><code>MultiGridBarrierMPI.fem2d_mpi_solve</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">fem2d_mpi_solve(::Type{T}=Float64; kwargs...) where {T}</code></pre><p><strong>Collective</strong></p><p>Solve a fem2d problem using amgb with MPI distributed types.</p><p>This is a convenience function that combines <code>fem2d_mpi</code> and <code>amgb</code> into a single call. It creates an MPI-based geometry and solves the barrier problem.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>: Element type for the geometry (default: Float64)</li><li><code>kwargs...</code>: Keyword arguments passed to both <code>fem2d_mpi</code> and <code>amgb</code><ul><li><code>maxh</code>: Maximum mesh size (passed to fem2d)</li><li><code>p</code>: Power parameter for the barrier (passed to amgb)</li><li><code>verbose</code>: Verbosity flag (passed to amgb)</li><li>Other arguments specific to fem2d or amgb</li></ul></li></ul><p><strong>Returns</strong></p><p>The solution object from <code>amgb</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">sol = fem2d_mpi_solve(Float64; maxh=0.1, p=2.0, verbose=true)
println(&quot;Solution norm: &quot;, norm(sol.z))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L493-L519">source</a></section></details></article><h3 id="3D-Problems"><a class="docs-heading-anchor" href="#3D-Problems">3D Problems</a><a id="3D-Problems-1"></a><a class="docs-heading-anchor-permalink" href="#3D-Problems" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="MultiGridBarrierMPI.fem3d_mpi"><a class="docstring-binding" href="#MultiGridBarrierMPI.fem3d_mpi"><code>MultiGridBarrierMPI.fem3d_mpi</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">fem3d_mpi(::Type{T}=Float64; kwargs...) where {T}</code></pre><p><strong>Collective</strong></p><p>Create an MPI-based Geometry from fem3d parameters.</p><p>This function calls <code>fem3d(kwargs...)</code> to create a native 3D geometry, then converts it to use MPI distributed types for distributed computing.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>: Element type for the geometry (default: Float64)</li><li><code>kwargs...</code>: Additional keyword arguments passed to <code>fem3d()</code>:<ul><li><code>L::Int</code>: Number of multigrid levels (default: 2)</li><li><code>k::Int</code>: Polynomial order of elements (default: 3)</li><li><code>K</code>: Coarse Q1 mesh as an N×3 matrix (optional, defaults to unit cube)</li></ul></li></ul><p><strong>Returns</strong></p><p>A Geometry object with MPI distributed types.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using MPI; MPI.Init()
using MultiGridBarrierMPI
g = fem3d_mpi(Float64; L=2, k=3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L528-L554">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MultiGridBarrierMPI.fem3d_mpi_solve"><a class="docstring-binding" href="#MultiGridBarrierMPI.fem3d_mpi_solve"><code>MultiGridBarrierMPI.fem3d_mpi_solve</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">fem3d_mpi_solve(::Type{T}=Float64; kwargs...) where {T}</code></pre><p><strong>Collective</strong></p><p>Solve a fem3d problem using amgb with MPI distributed types.</p><p>This is a convenience function that combines <code>fem3d_mpi</code> and <code>amgb</code> into a single call. It creates an MPI-based 3D geometry and solves the barrier problem.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type</code>: Element type for the geometry (default: Float64)</li><li><code>kwargs...</code>: Keyword arguments passed to both <code>fem3d_mpi</code> and <code>amgb</code><ul><li><code>L::Int</code>: Number of multigrid levels (passed to fem3d)</li><li><code>k::Int</code>: Polynomial order of elements (passed to fem3d)</li><li><code>p</code>: Power parameter for the barrier (passed to amgb)</li><li><code>verbose</code>: Verbosity flag (passed to amgb)</li><li><code>D</code>: Operator structure matrix (passed to amgb, defaults to 3D operators)</li><li><code>f</code>: Source term function (passed to amgb, defaults to 3D source)</li><li><code>g</code>: Boundary condition function (passed to amgb, defaults to 3D BCs)</li><li>Other arguments specific to fem3d or amgb</li></ul></li></ul><p><strong>Returns</strong></p><p>The solution object from <code>amgb</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">sol = fem3d_mpi_solve(Float64; L=2, k=3, p=1.0, verbose=true)
println(&quot;Solution norm: &quot;, norm(sol.z))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L563-L593">source</a></section></details></article><h2 id="Type-Conversion-API"><a class="docs-heading-anchor" href="#Type-Conversion-API">Type Conversion API</a><a id="Type-Conversion-API-1"></a><a class="docs-heading-anchor-permalink" href="#Type-Conversion-API" title="Permalink"></a></h2><p>These functions convert between native Julia types and MPI distributed types. The <code>mpi_to_native</code> function dispatches on type, handling <code>Geometry</code>, <code>AMGBSOL</code>, and <code>ParabolicSOL</code> objects.</p><article><details class="docstring" open="true"><summary id="MultiGridBarrierMPI.native_to_mpi"><a class="docstring-binding" href="#MultiGridBarrierMPI.native_to_mpi"><code>MultiGridBarrierMPI.native_to_mpi</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">native_to_mpi(g_native::Geometry{T, Matrix{T}, Vector{T}, SparseMatrixCSC{T,Int}, Discretization}) where {T, Discretization}</code></pre><p><strong>Collective</strong></p><p>Convert a native Geometry object (with Julia arrays) to use MPI distributed types.</p><p>This is a collective operation. Each rank calls fem2d() to get the same native geometry, then this function converts:</p><ul><li>x::Matrix{T} -&gt; x::MatrixMPI{T}</li><li>w::Vector{T} -&gt; w::VectorMPI{T}</li><li>operators[key]::SparseMatrixCSC{T,Int} -&gt; operators[key]::SparseMatrixMPI{T}</li><li>subspaces[key][i]::SparseMatrixCSC{T,Int} -&gt; subspaces[key][i]::SparseMatrixMPI{T}</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L128-L141">source</a></section></details></article><article><details class="docstring" open="true"><summary id="MultiGridBarrierMPI.mpi_to_native"><a class="docstring-binding" href="#MultiGridBarrierMPI.mpi_to_native"><code>MultiGridBarrierMPI.mpi_to_native</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mpi_to_native(g_mpi::Geometry{T, MatrixMPI{T}, VectorMPI{T}, &lt;:SparseMatrixMPI{T}, Discretization}) where {T, Discretization}</code></pre><p><strong>Collective</strong></p><p>Convert an MPI Geometry object (with distributed MPI types) back to native Julia arrays.</p><p>This is a collective operation. This function converts:</p><ul><li>x::MatrixMPI{T} -&gt; x::Matrix{T}</li><li>w::VectorMPI{T} -&gt; w::Vector{T}</li><li>operators[key]::SparseMatrixMPI{T} -&gt; operators[key]::SparseMatrixCSC{T,Int}</li><li>subspaces[key][i]::SparseMatrixMPI{T} -&gt; subspaces[key][i]::SparseMatrixCSC{T,Int}</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L212-L224">source</a></section><section><div><pre><code class="language-julia hljs">mpi_to_native(sol_mpi::AMGBSOL{T, XType, WType, MType, Discretization}) where {T, XType, WType, MType, Discretization}</code></pre><p><strong>Collective</strong></p><p>Convert an AMGBSOL solution object from MPI types back to native Julia types.</p><p>This is a collective operation that performs a deep conversion of the solution structure:</p><ul><li>z: MatrixMPI{T} -&gt; Matrix{T} or VectorMPI{T} -&gt; Vector{T}</li><li>SOL_feasibility: NamedTuple with MPI types -&gt; NamedTuple with native types</li><li>SOL_main: NamedTuple with MPI types -&gt; NamedTuple with native types</li><li>geometry: Geometry with MPI types -&gt; Geometry with native types</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L275-L287">source</a></section><section><div><pre><code class="language-julia hljs">mpi_to_native(sol_mpi::ParabolicSOL{T, XType, WType, MType, Discretization}) where {T, XType, WType, MType, Discretization}</code></pre><p><strong>Collective</strong></p><p>Convert a ParabolicSOL solution object from MPI types back to native Julia types.</p><p>This is a collective operation that performs a deep conversion of the parabolic solution:</p><ul><li>geometry: Geometry with MPI types -&gt; Geometry with native types</li><li>ts: Vector{T} (unchanged, already native)</li><li>u: Vector{MatrixMPI{T}} -&gt; Vector{Matrix{T}} (each time snapshot converted)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">g = fem2d_mpi(Float64; L=2)
sol_mpi = parabolic_solve(g; h=0.5, p=1.0)
sol_native = mpi_to_native(sol_mpi)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/MultiGridBarrierMPI.jl/blob/3dd41edd2dda2efa18c4d1530575ff1db354043c/src/MultiGridBarrierMPI.jl#L339-L357">source</a></section></details></article><h2 id="Type-Mappings-Reference"><a class="docs-heading-anchor" href="#Type-Mappings-Reference">Type Mappings Reference</a><a id="Type-Mappings-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Type-Mappings-Reference" title="Permalink"></a></h2><h3 id="Native-to-MPI-Conversions"><a class="docs-heading-anchor" href="#Native-to-MPI-Conversions">Native to MPI Conversions</a><a id="Native-to-MPI-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#Native-to-MPI-Conversions" title="Permalink"></a></h3><p>When converting from native Julia types to MPI distributed types:</p><table><tr><th style="text-align: right">Native Type</th><th style="text-align: right">MPI Type</th><th style="text-align: right">Usage</th></tr><tr><td style="text-align: right"><code>Matrix{T}</code></td><td style="text-align: right"><code>MatrixMPI{T}</code></td><td style="text-align: right">Geometry coordinates, dense data</td></tr><tr><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right"><code>VectorMPI{T}</code></td><td style="text-align: right">Weights, dense vectors</td></tr><tr><td style="text-align: right"><code>SparseMatrixCSC{T,Int}</code></td><td style="text-align: right"><code>SparseMatrixMPI{T}</code></td><td style="text-align: right">Sparse operators, subspace matrices</td></tr></table><h3 id="MPI-to-Native-Conversions"><a class="docs-heading-anchor" href="#MPI-to-Native-Conversions">MPI to Native Conversions</a><a id="MPI-to-Native-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-to-Native-Conversions" title="Permalink"></a></h3><p>When converting from MPI distributed types back to native Julia types:</p><table><tr><th style="text-align: right">MPI Type</th><th style="text-align: right">Native Type</th></tr><tr><td style="text-align: right"><code>MatrixMPI{T}</code></td><td style="text-align: right"><code>Matrix{T}</code></td></tr><tr><td style="text-align: right"><code>VectorMPI{T}</code></td><td style="text-align: right"><code>Vector{T}</code></td></tr><tr><td style="text-align: right"><code>SparseMatrixMPI{T}</code></td><td style="text-align: right"><code>SparseMatrixCSC{T,Int}</code></td></tr></table><h2 id="Geometry-Structure"><a class="docs-heading-anchor" href="#Geometry-Structure">Geometry Structure</a><a id="Geometry-Structure-1"></a><a class="docs-heading-anchor-permalink" href="#Geometry-Structure" title="Permalink"></a></h2><p>The <code>Geometry</code> type from MultiGridBarrier is parameterized by its storage types:</p><p><strong>Native Geometry:</strong></p><pre><code class="language-julia hljs">Geometry{T, Matrix{T}, Vector{T}, SparseMatrixCSC{T,Int}, Discretization}</code></pre><p><strong>MPI Geometry:</strong></p><pre><code class="language-julia hljs">Geometry{T, MatrixMPI{T}, VectorMPI{T}, SparseMatrixMPI{T}, Discretization}</code></pre><h3 id="Fields"><a class="docs-heading-anchor" href="#Fields">Fields</a><a id="Fields-1"></a><a class="docs-heading-anchor-permalink" href="#Fields" title="Permalink"></a></h3><ul><li><strong><code>discretization</code></strong>: Discretization information (domain, mesh, etc.)</li><li><strong><code>x</code></strong>: Geometry coordinates (Matrix or MatrixMPI)</li><li><strong><code>w</code></strong>: Quadrature weights (Vector or VectorMPI)</li><li><strong><code>operators</code></strong>: Dictionary of operators (id, dx, dy, etc.)</li><li><strong><code>subspaces</code></strong>: Dictionary of subspace projection matrices</li><li><strong><code>refine</code></strong>: Vector of refinement matrices (coarse -&gt; fine)</li><li><strong><code>coarsen</code></strong>: Vector of coarsening matrices (fine -&gt; coarse)</li></ul><h2 id="Solution-Structure"><a class="docs-heading-anchor" href="#Solution-Structure">Solution Structure</a><a id="Solution-Structure-1"></a><a class="docs-heading-anchor-permalink" href="#Solution-Structure" title="Permalink"></a></h2><p>The <code>AMGBSOL</code> type from MultiGridBarrier contains the complete solution:</p><h3 id="Fields-2"><a class="docs-heading-anchor" href="#Fields-2">Fields</a><a class="docs-heading-anchor-permalink" href="#Fields-2" title="Permalink"></a></h3><ul><li><strong><code>z</code></strong>: Solution matrix/vector</li><li><strong><code>SOL_feasibility</code></strong>: NamedTuple with feasibility phase information</li><li><strong><code>SOL_main</code></strong>: NamedTuple with main solve information<ul><li><code>t_elapsed</code>: Elapsed solve time in seconds</li><li><code>ts</code>: Barrier parameter values</li><li><code>its</code>: Iterations per level</li><li><code>c_dot_Dz</code>: Convergence measure values</li></ul></li><li><strong><code>log</code></strong>: Vector of iteration logs</li><li><strong><code>geometry</code></strong>: The geometry used for solving</li></ul><h2 id="MPI-and-IO-Utilities"><a class="docs-heading-anchor" href="#MPI-and-IO-Utilities">MPI and IO Utilities</a><a id="MPI-and-IO-Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-and-IO-Utilities" title="Permalink"></a></h2><h3 id="LinearAlgebraMPI.io0()"><a class="docs-heading-anchor" href="#LinearAlgebraMPI.io0()">LinearAlgebraMPI.io0()</a><a id="LinearAlgebraMPI.io0()-1"></a><a class="docs-heading-anchor-permalink" href="#LinearAlgebraMPI.io0()" title="Permalink"></a></h3><p>Returns an IO stream that only writes on rank 0:</p><pre><code class="language-julia hljs">using LinearAlgebraMPI

println(io0(), &quot;This prints once from rank 0&quot;)</code></pre><h3 id="MPI-Rank-Information"><a class="docs-heading-anchor" href="#MPI-Rank-Information">MPI Rank Information</a><a id="MPI-Rank-Information-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-Rank-Information" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI

rank = MPI.Comm_rank(MPI.COMM_WORLD)  # Current rank (0 to nranks-1)
nranks = MPI.Comm_size(MPI.COMM_WORLD)  # Total number of ranks</code></pre><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><h3 id="Type-Conversion-Round-Trip"><a class="docs-heading-anchor" href="#Type-Conversion-Round-Trip">Type Conversion Round-Trip</a><a id="Type-Conversion-Round-Trip-1"></a><a class="docs-heading-anchor-permalink" href="#Type-Conversion-Round-Trip" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI
MPI.Init()

using MultiGridBarrierMPI
using LinearAlgebraMPI
using MultiGridBarrier
using LinearAlgebra

# Create native geometry
g_native = fem2d(; maxh=0.3)

# Convert to MPI
g_mpi = native_to_mpi(g_native)

# Solve with MPI types
sol_mpi = amgb(g_mpi; p=2.0)

# Convert back to native
sol_native = mpi_to_native(sol_mpi)
g_back = mpi_to_native(g_mpi)

# Verify round-trip accuracy
@assert norm(g_native.x - g_back.x) &lt; 1e-10
@assert norm(g_native.w - g_back.w) &lt; 1e-10</code></pre><h3 id="Accessing-Operator-Matrices"><a class="docs-heading-anchor" href="#Accessing-Operator-Matrices">Accessing Operator Matrices</a><a id="Accessing-Operator-Matrices-1"></a><a class="docs-heading-anchor-permalink" href="#Accessing-Operator-Matrices" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Native geometry
g_native = fem2d(; maxh=0.2)
id_native = g_native.operators[:id]  # SparseMatrixCSC

# MPI geometry
g_mpi = native_to_mpi(g_native)
id_mpi = g_mpi.operators[:id]  # SparseMatrixMPI

# Convert back if needed
id_back = SparseMatrixCSC(id_mpi)  # SparseMatrixCSC</code></pre><h2 id="Integration-with-MultiGridBarrier"><a class="docs-heading-anchor" href="#Integration-with-MultiGridBarrier">Integration with MultiGridBarrier</a><a id="Integration-with-MultiGridBarrier-1"></a><a class="docs-heading-anchor-permalink" href="#Integration-with-MultiGridBarrier" title="Permalink"></a></h2><p>All MultiGridBarrier functions work seamlessly with MPI types:</p><pre><code class="language-julia hljs">using MultiGridBarrier: amgb

# Create MPI geometry
g = fem2d_mpi(Float64; L=3)

# Use MultiGridBarrier functions directly
sol = amgb(g; p=1.0, verbose=true)</code></pre><p>The package extends MultiGridBarrier&#39;s internal API (<code>amgb_zeros</code>, <code>amgb_diag</code>, <code>amgb_blockdiag</code>, <code>map_rows</code>, etc.) to work with MPI types automatically.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../guide/">« User Guide</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 19 December 2025 14:13">Friday 19 December 2025</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
