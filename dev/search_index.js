var documenterSearchIndex = {"docs":
[{"location":"guide/#User-Guide","page":"User Guide","title":"User Guide","text":"This guide covers the essential workflows for using MultiGridBarrierMPI.jl.","category":"section"},{"location":"guide/#Initialization","page":"User Guide","title":"Initialization","text":"Every program using MultiGridBarrierMPI.jl must initialize MPI before using the package:\n\nusing MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI","category":"section"},{"location":"guide/#Basic-Workflow","page":"User Guide","title":"Basic Workflow","text":"The typical workflow consists of three steps:\n\nSolve with MPI types (distributed computation)\nConvert to native types (for analysis/plotting)\nVisualize or analyze (using MultiGridBarrier's tools)","category":"section"},{"location":"guide/#Complete-Example-with-Visualization","page":"User Guide","title":"Complete Example with Visualization","text":"Here's a complete example that solves a 2D FEM problem, converts the solution, and plots it:\n\nusing MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\nusing MultiGridBarrier\n\n# Step 1: Solve with MPI distributed types\nsol_mpi = fem2d_mpi_solve(Float64; L=3, p=1.0, verbose=false)\n\n# Step 2: Convert solution to native Julia types\nsol_native = mpi_to_native(sol_mpi)\n\n# Step 3: Plot the solution using MultiGridBarrier's plot function\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nif rank == 0\n    using PyPlot\n    figure(figsize=(10, 8))\n    plot(sol_native)\n    title(\"Multigrid Barrier Solution (L=3)\")\n    tight_layout()\n    savefig(\"solution_plot.png\")\nend\nprintln(io0(), \"Solution plotted!\")\n\ntip: Running This Example\nSave this code to a file (e.g., visualize.jl) and run with:mpiexec -n 4 julia --project visualize.jl","category":"section"},{"location":"guide/#Understanding-MPI-Collective-Operations","page":"User Guide","title":"Understanding MPI Collective Operations","text":"warning: All Functions Are Collective\nAll exported functions in MultiGridBarrierMPI.jl are MPI collective operations. This means:All MPI ranks must call the function\nAll ranks must call it with the same parameters\nDeadlock will occur if only some ranks call a collective function\n\nCorrect usage:\n\n# All ranks execute this together\nsol = fem2d_mpi_solve(Float64; L=2, p=1.0)\n\nIncorrect usage (causes deadlock):\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nif rank == 0\n    sol = fem2d_mpi_solve(Float64; L=2, p=1.0)  # Only rank 0 calls - DEADLOCK!\nend","category":"section"},{"location":"guide/#Type-Conversions","page":"User Guide","title":"Type Conversions","text":"","category":"section"},{"location":"guide/#Native-to-MPI","page":"User Guide","title":"Native to MPI","text":"Convert native Julia arrays to MPI distributed types:\n\nusing MultiGridBarrier\n\n# Create native geometry\ng_native = fem2d(; L=2)\n\n# Convert to MPI types for distributed computation\ng_mpi = native_to_mpi(g_native)\n\nType mappings:\n\nNative Type MPI Type Description\nMatrix{T} MatrixMPI{T} Dense distributed matrix\nVector{T} VectorMPI{T} Dense distributed vector\nSparseMatrixCSC{T,Int} SparseMatrixMPI{T} Sparse distributed matrix","category":"section"},{"location":"guide/#MPI-to-Native","page":"User Guide","title":"MPI to Native","text":"Convert MPI types back to native Julia arrays:\n\n# Create and solve with MPI types\ng_mpi = fem2d_mpi(Float64; L=2)\nsol_mpi = amgb(g_mpi; p=2.0)\n\n# Convert back for analysis\ng_native = mpi_to_native(g_mpi)\nsol_native = mpi_to_native(sol_mpi)\n\n# Now you can use native Julia operations\nusing LinearAlgebra\nz_matrix = sol_native.z\nsolution_norm = norm(z_matrix)\nprintln(io0(), \"Solution norm: \", solution_norm)","category":"section"},{"location":"guide/#Advanced-Usage","page":"User Guide","title":"Advanced Usage","text":"","category":"section"},{"location":"guide/#Custom-Geometry-Workflow","page":"User Guide","title":"Custom Geometry Workflow","text":"For more control, construct geometries manually:\n\nusing MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\nusing MultiGridBarrier\n\n# 1. Create native geometry with specific parameters\ng_native = fem2d(; L=2)\n\n# 2. Convert to MPI for distributed solving\ng_mpi = native_to_mpi(g_native)\n\n# 3. Solve with custom barrier parameters\nsol_mpi = amgb(g_mpi;\n    p=1.5,           # Barrier power parameter\n    verbose=true,    # Print convergence info\n    maxit=100,       # Maximum iterations\n    tol=1e-8)        # Convergence tolerance\n\n# 4. Convert solution back\nsol_native = mpi_to_native(sol_mpi)\n\n# 5. Access solution components\nprintln(io0(), \"Newton steps: \", sum(sol_native.SOL_main.its))\nprintln(io0(), \"Elapsed time: \", sol_native.SOL_main.t_elapsed, \" seconds\")","category":"section"},{"location":"guide/#Comparing-MPI-vs-Native-Solutions","page":"User Guide","title":"Comparing MPI vs Native Solutions","text":"Verify that MPI and native implementations give the same results:\n\nusing MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\nusing MultiGridBarrier\nusing LinearAlgebra\n\n# Solve with MPI (distributed)\nsol_mpi_dist = fem2d_mpi_solve(Float64; L=2, p=1.0, verbose=false)\nz_mpi = mpi_to_native(sol_mpi_dist).z\n\n# Solve with native (sequential, on rank 0)\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nif rank == 0\n    sol_native = MultiGridBarrier.fem2d_solve(Float64; L=2, p=1.0, verbose=false)\n    z_native = sol_native.z\n\n    # Compare solutions\n    diff = norm(z_mpi - z_native) / norm(z_native)\n    println(\"Relative difference: \", diff)\n    @assert diff < 1e-10 \"Solutions should match!\"\nend","category":"section"},{"location":"guide/#IO-and-Output","page":"User Guide","title":"IO and Output","text":"","category":"section"},{"location":"guide/#Printing-from-One-Rank","page":"User Guide","title":"Printing from One Rank","text":"Use io0() from LinearAlgebraMPI to print from rank 0 only:\n\nusing LinearAlgebraMPI\n\n# This prints once (from rank 0)\nprintln(io0(), \"Hello from rank 0!\")\n\n# Without io0(), this prints from ALL ranks\nprintln(\"Hello from rank \", MPI.Comm_rank(MPI.COMM_WORLD))","category":"section"},{"location":"guide/#MPI-Rank-Information","page":"User Guide","title":"MPI Rank Information","text":"using MPI\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)  # Current rank (0 to nranks-1)\nnranks = MPI.Comm_size(MPI.COMM_WORLD)  # Total number of ranks","category":"section"},{"location":"guide/#Performance-Considerations","page":"User Guide","title":"Performance Considerations","text":"","category":"section"},{"location":"guide/#Threading","page":"User Guide","title":"Threading","text":"For optimal performance, use BLAS threading with a single Julia thread:\n\nexport OMP_NUM_THREADS=1\nexport OPENBLAS_NUM_THREADS=10  # or your number of CPU cores\nmpiexec -n 1 julia --project my_program.jl\n\nBLAS threads (OPENBLAS_NUM_THREADS) - Controls threading for dense matrix operations in both Julia and MUMPS\nOpenMP threads (OMP_NUM_THREADS) - Should be set to 1; MUMPS uses BLAS threading internally\n\nYou can also set these in Julia's startup.jl:\n\n# In ~/.julia/config/startup.jl\nENV[\"OMP_NUM_THREADS\"] = \"1\"\nENV[\"OPENBLAS_NUM_THREADS\"] = string(Sys.CPU_THREADS)","category":"section"},{"location":"guide/#Performance-Comparison-(Single-Rank)","page":"User Guide","title":"Performance Comparison (Single-Rank)","text":"The following table compares MultiGridBarrierMPI (using MUMPS) against MultiGridBarrier.jl's native solver on a 2D p-Laplace problem. Both use OMP_NUM_THREADS=1 and OPENBLAS_NUM_THREADS=10. Benchmarks were run on a 2025 M4 MacBook Pro with 10 CPU cores:\n\nL n (grid points) Native (s) MPI (s) Ratio Diff\n1 14 0.018 0.029 1.60x 0.00e+00\n2 56 0.028 0.039 1.41x 8.88e-16\n3 224 0.074 0.078 1.06x 4.13e-11\n4 896 0.480 0.410 0.85x 1.04e-14\n5 3,584 2.508 1.771 0.71x 3.55e-12\n6 14,336 26.384 68.846 2.61x 1.08e-13\n7 57,344 96.694 118.070 1.22x 3.31e-13\n8 229,376 659.276 504.672 0.77x 1.31e-11\n\nRatio = MPI time / Native time (lower is better, <1.0 means MPI is faster) Diff = sup-norm difference between native and MPI solutions\n\nFor medium-sized problems (L=4-5) and large problems (L=8), the MPI version is faster than the native solver.","category":"section"},{"location":"guide/#1D-Problems","page":"User Guide","title":"1D Problems","text":"MultiGridBarrierMPI supports 1D finite element problems.","category":"section"},{"location":"guide/#Basic-1D-Example","page":"User Guide","title":"Basic 1D Example","text":"using MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\n\n# Solve a 1D problem with 4 multigrid levels (2^4 = 16 elements)\nsol = fem1d_mpi_solve(Float64; L=4, p=1.0, verbose=true)\n\n# Convert solution to native types for analysis\nsol_native = mpi_to_native(sol)\n\nprintln(io0(), \"Solution computed successfully!\")\nprintln(io0(), \"Newton steps: \", sum(sol_native.SOL_main.its))","category":"section"},{"location":"guide/#1D-Parameters","page":"User Guide","title":"1D Parameters","text":"The fem1d_mpi and fem1d_mpi_solve functions accept:\n\nParameter Description Default\nL Number of multigrid levels (creates 2^L elements) 4","category":"section"},{"location":"guide/#2D-Problems","page":"User Guide","title":"2D Problems","text":"","category":"section"},{"location":"guide/#Basic-2D-Example","page":"User Guide","title":"Basic 2D Example","text":"using MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\n\n# Solve a 2D problem\nsol = fem2d_mpi_solve(Float64; L=2, p=1.0, verbose=true)\n\n# Convert solution to native types for analysis\nsol_native = mpi_to_native(sol)\n\nprintln(io0(), \"Solution computed successfully!\")\nprintln(io0(), \"Newton steps: \", sum(sol_native.SOL_main.its))","category":"section"},{"location":"guide/#2D-Parameters","page":"User Guide","title":"2D Parameters","text":"Parameter Description Default\nL Number of multigrid levels 2\nK Coarse mesh vertices (3n×2 matrix) Unit square triangulation","category":"section"},{"location":"guide/#3D-Problems","page":"User Guide","title":"3D Problems","text":"MultiGridBarrierMPI supports 3D hexahedral finite elements.","category":"section"},{"location":"guide/#Basic-3D-Example","page":"User Guide","title":"Basic 3D Example","text":"using MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\n\n# Solve a 3D problem with Q3 elements and 2 multigrid levels\nsol = fem3d_mpi_solve(Float64; L=2, k=3, p=1.0, verbose=true)\n\n# Convert solution to native types for analysis\nsol_native = mpi_to_native(sol)\n\nprintln(io0(), \"Solution computed successfully!\")\nprintln(io0(), \"Newton steps: \", sum(sol_native.SOL_main.its))","category":"section"},{"location":"guide/#3D-Parameters","page":"User Guide","title":"3D Parameters","text":"Parameter Description Default\nL Number of multigrid levels 2\nk Polynomial order of elements (Q_k) 3","category":"section"},{"location":"guide/#Time-Dependent-(Parabolic)-Problems","page":"User Guide","title":"Time-Dependent (Parabolic) Problems","text":"MultiGridBarrierMPI supports time-dependent parabolic PDEs through MultiGridBarrier.jl's parabolic_solve function.","category":"section"},{"location":"guide/#Basic-Parabolic-Example","page":"User Guide","title":"Basic Parabolic Example","text":"using MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\nusing MultiGridBarrier\n\n# Create MPI geometry\ng = fem2d_mpi(Float64; L=2)\n\n# Solve time-dependent problem from t=0 to t=1 with timestep h=0.2\nsol = parabolic_solve(g; h=0.2, p=1.0, verbose=true)\n\nprintln(io0(), \"Parabolic solve completed!\")\nprintln(io0(), \"Number of timesteps: \", length(sol.ts))","category":"section"},{"location":"guide/#Converting-Parabolic-Solutions-to-Native-Types","page":"User Guide","title":"Converting Parabolic Solutions to Native Types","text":"g = fem2d_mpi(Float64; L=2)\nsol_mpi = parabolic_solve(g; h=0.25, p=1.0, verbose=false)\n\n# Convert to native types\nsol_native = mpi_to_native(sol_mpi)\n\n# Now sol_native.u contains Vector{Matrix{Float64}}\nprintln(io0(), \"Native u type: \", typeof(sol_native.u))\nprintln(io0(), \"Snapshot size: \", size(sol_native.u[1]))","category":"section"},{"location":"guide/#Common-Patterns","page":"User Guide","title":"Common Patterns","text":"","category":"section"},{"location":"guide/#Solve-and-Extract-Specific-Values","page":"User Guide","title":"Solve and Extract Specific Values","text":"using MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\n\nsol = fem2d_mpi_solve(Float64; L=3, p=1.0)\nsol_native = mpi_to_native(sol)\n\n# Access solution data\nz = sol_native.z  # Solution matrix\niters = sum(sol_native.SOL_main.its)  # Total Newton steps\nelapsed = sol_native.SOL_main.t_elapsed  # Elapsed time in seconds\n\nprintln(io0(), \"Converged in $iters iterations\")\nprintln(io0(), \"Elapsed time: $elapsed seconds\")","category":"section"},{"location":"guide/#Next-Steps","page":"User Guide","title":"Next Steps","text":"See the API Reference for detailed function documentation\nCheck the examples/ directory for complete runnable examples\nConsult MultiGridBarrier.jl documentation for barrier method theory","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"This page provides detailed documentation for all exported functions in MultiGridBarrierMPI.jl.\n\nnote: All Functions Are Collective\nAll functions documented here are MPI collective operations. Every MPI rank must call these functions together with the same parameters. Failure to do so will result in deadlock.","category":"section"},{"location":"api/#High-Level-API","page":"API Reference","title":"High-Level API","text":"These functions provide the simplest interface for solving problems with MPI types.","category":"section"},{"location":"api/#1D-Problems","page":"API Reference","title":"1D Problems","text":"","category":"section"},{"location":"api/#2D-Problems","page":"API Reference","title":"2D Problems","text":"","category":"section"},{"location":"api/#3D-Problems","page":"API Reference","title":"3D Problems","text":"","category":"section"},{"location":"api/#Type-Conversion-API","page":"API Reference","title":"Type Conversion API","text":"These functions convert between native Julia types and MPI distributed types. The mpi_to_native function dispatches on type, handling Geometry, AMGBSOL, and ParabolicSOL objects.","category":"section"},{"location":"api/#Type-Mappings-Reference","page":"API Reference","title":"Type Mappings Reference","text":"","category":"section"},{"location":"api/#Native-to-MPI-Conversions","page":"API Reference","title":"Native to MPI Conversions","text":"When converting from native Julia types to MPI distributed types:\n\nNative Type MPI Type Usage\nMatrix{T} MatrixMPI{T} Geometry coordinates, dense data\nVector{T} VectorMPI{T} Weights, dense vectors\nSparseMatrixCSC{T,Int} SparseMatrixMPI{T} Sparse operators, subspace matrices","category":"section"},{"location":"api/#MPI-to-Native-Conversions","page":"API Reference","title":"MPI to Native Conversions","text":"When converting from MPI distributed types back to native Julia types:\n\nMPI Type Native Type\nMatrixMPI{T} Matrix{T}\nVectorMPI{T} Vector{T}\nSparseMatrixMPI{T} SparseMatrixCSC{T,Int}","category":"section"},{"location":"api/#Geometry-Structure","page":"API Reference","title":"Geometry Structure","text":"The Geometry type from MultiGridBarrier is parameterized by its storage types:\n\nNative Geometry:\n\nGeometry{T, Matrix{T}, Vector{T}, SparseMatrixCSC{T,Int}, Discretization}\n\nMPI Geometry:\n\nGeometry{T, MatrixMPI{T}, VectorMPI{T}, SparseMatrixMPI{T}, Discretization}","category":"section"},{"location":"api/#Fields","page":"API Reference","title":"Fields","text":"discretization: Discretization information (domain, mesh, etc.)\nx: Geometry coordinates (Matrix or MatrixMPI)\nw: Quadrature weights (Vector or VectorMPI)\noperators: Dictionary of operators (id, dx, dy, etc.)\nsubspaces: Dictionary of subspace projection matrices\nrefine: Vector of refinement matrices (coarse -> fine)\ncoarsen: Vector of coarsening matrices (fine -> coarse)","category":"section"},{"location":"api/#Solution-Structure","page":"API Reference","title":"Solution Structure","text":"The AMGBSOL type from MultiGridBarrier contains the complete solution:","category":"section"},{"location":"api/#Fields-2","page":"API Reference","title":"Fields","text":"z: Solution matrix/vector\nSOL_feasibility: NamedTuple with feasibility phase information\nSOL_main: NamedTuple with main solve information\nt_elapsed: Elapsed solve time in seconds\nts: Barrier parameter values\nits: Iterations per level\nc_dot_Dz: Convergence measure values\nlog: Vector of iteration logs\ngeometry: The geometry used for solving","category":"section"},{"location":"api/#MPI-and-IO-Utilities","page":"API Reference","title":"MPI and IO Utilities","text":"","category":"section"},{"location":"api/#LinearAlgebraMPI.io0()","page":"API Reference","title":"LinearAlgebraMPI.io0()","text":"Returns an IO stream that only writes on rank 0:\n\nusing LinearAlgebraMPI\n\nprintln(io0(), \"This prints once from rank 0\")","category":"section"},{"location":"api/#MPI-Rank-Information","page":"API Reference","title":"MPI Rank Information","text":"using MPI\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)  # Current rank (0 to nranks-1)\nnranks = MPI.Comm_size(MPI.COMM_WORLD)  # Total number of ranks","category":"section"},{"location":"api/#Examples","page":"API Reference","title":"Examples","text":"","category":"section"},{"location":"api/#Type-Conversion-Round-Trip","page":"API Reference","title":"Type Conversion Round-Trip","text":"using MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\nusing MultiGridBarrier\nusing LinearAlgebra\n\n# Create native geometry\ng_native = fem2d(; L=2)\n\n# Convert to MPI\ng_mpi = native_to_mpi(g_native)\n\n# Solve with MPI types\nsol_mpi = amgb(g_mpi; p=2.0)\n\n# Convert back to native\nsol_native = mpi_to_native(sol_mpi)\ng_back = mpi_to_native(g_mpi)\n\n# Verify round-trip accuracy\n@assert norm(g_native.x - g_back.x) < 1e-10\n@assert norm(g_native.w - g_back.w) < 1e-10","category":"section"},{"location":"api/#Accessing-Operator-Matrices","page":"API Reference","title":"Accessing Operator Matrices","text":"# Native geometry\ng_native = fem2d(; L=2)\nid_native = g_native.operators[:id]  # SparseMatrixCSC\n\n# MPI geometry\ng_mpi = native_to_mpi(g_native)\nid_mpi = g_mpi.operators[:id]  # SparseMatrixMPI\n\n# Convert back if needed\nid_back = SparseMatrixCSC(id_mpi)  # SparseMatrixCSC","category":"section"},{"location":"api/#Integration-with-MultiGridBarrier","page":"API Reference","title":"Integration with MultiGridBarrier","text":"All MultiGridBarrier functions work seamlessly with MPI types:\n\nusing MultiGridBarrier: amgb\n\n# Create MPI geometry\ng = fem2d_mpi(Float64; L=3)\n\n# Use MultiGridBarrier functions directly\nsol = amgb(g; p=1.0, verbose=true)\n\nThe package extends MultiGridBarrier's internal API (amgb_zeros, amgb_diag, amgb_blockdiag, map_rows, etc.) to work with MPI types automatically.","category":"section"},{"location":"api/#MultiGridBarrierMPI.fem1d_mpi","page":"API Reference","title":"MultiGridBarrierMPI.fem1d_mpi","text":"fem1d_mpi(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nCreate an MPI-based Geometry from fem1d parameters.\n\nThis function calls fem1d(kwargs...) to create a native 1D geometry, then converts it to use MPI distributed types for distributed computing.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Additional keyword arguments passed to fem1d():\nL::Int: Number of multigrid levels (default: 4), creating 2^L elements\n\nReturns\n\nA Geometry object with MPI distributed types.\n\nExample\n\nusing MPI; MPI.Init()\nusing MultiGridBarrierMPI\ng = fem1d_mpi(Float64; L=4)\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierMPI.fem1d_mpi_solve","page":"API Reference","title":"MultiGridBarrierMPI.fem1d_mpi_solve","text":"fem1d_mpi_solve(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nSolve a fem1d problem using amgb with MPI distributed types.\n\nThis is a convenience function that combines fem1d_mpi and amgb into a single call. It creates an MPI-based 1D geometry and solves the barrier problem.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Keyword arguments passed to both fem1d_mpi and amgb\nL::Int: Number of multigrid levels (passed to fem1d)\np: Power parameter for the barrier (passed to amgb)\nverbose: Verbosity flag (passed to amgb)\nOther arguments specific to fem1d or amgb\n\nReturns\n\nThe solution object from amgb.\n\nExample\n\nsol = fem1d_mpi_solve(Float64; L=4, p=1.0, verbose=true)\nprintln(\"Solution norm: \", norm(sol.z))\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierMPI.fem2d_mpi","page":"API Reference","title":"MultiGridBarrierMPI.fem2d_mpi","text":"fem2d_mpi(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nCreate an MPI-based Geometry from fem2d parameters.\n\nThis function calls fem2d(kwargs...) to create a native geometry, then converts it to use MPI distributed types for distributed computing.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Additional keyword arguments passed to fem2d()\n\nReturns\n\nA Geometry object with MPI distributed types.\n\nExample\n\nusing MPI; MPI.Init()\nusing MultiGridBarrierMPI\ng = fem2d_mpi(Float64; L=3)\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierMPI.fem2d_mpi_solve","page":"API Reference","title":"MultiGridBarrierMPI.fem2d_mpi_solve","text":"fem2d_mpi_solve(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nSolve a fem2d problem using amgb with MPI distributed types.\n\nThis is a convenience function that combines fem2d_mpi and amgb into a single call. It creates an MPI-based geometry and solves the barrier problem.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Keyword arguments passed to both fem2d_mpi and amgb\nL: Number of multigrid levels (passed to fem2d)\np: Power parameter for the barrier (passed to amgb)\nverbose: Verbosity flag (passed to amgb)\nOther arguments specific to fem2d or amgb\n\nReturns\n\nThe solution object from amgb.\n\nExample\n\nsol = fem2d_mpi_solve(Float64; L=3, p=2.0, verbose=true)\nprintln(\"Solution norm: \", norm(sol.z))\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierMPI.fem3d_mpi","page":"API Reference","title":"MultiGridBarrierMPI.fem3d_mpi","text":"fem3d_mpi(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nCreate an MPI-based Geometry from fem3d parameters.\n\nThis function calls fem3d(kwargs...) to create a native 3D geometry, then converts it to use MPI distributed types for distributed computing.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Additional keyword arguments passed to fem3d():\nL::Int: Number of multigrid levels (default: 2)\nk::Int: Polynomial order of elements (default: 3)\nK: Coarse Q1 mesh as an N×3 matrix (optional, defaults to unit cube)\n\nReturns\n\nA Geometry object with MPI distributed types.\n\nExample\n\nusing MPI; MPI.Init()\nusing MultiGridBarrierMPI\ng = fem3d_mpi(Float64; L=2, k=3)\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierMPI.fem3d_mpi_solve","page":"API Reference","title":"MultiGridBarrierMPI.fem3d_mpi_solve","text":"fem3d_mpi_solve(::Type{T}=Float64; kwargs...) where {T}\n\nCollective\n\nSolve a fem3d problem using amgb with MPI distributed types.\n\nThis is a convenience function that combines fem3d_mpi and amgb into a single call. It creates an MPI-based 3D geometry and solves the barrier problem.\n\nArguments\n\nT::Type: Element type for the geometry (default: Float64)\nkwargs...: Keyword arguments passed to both fem3d_mpi and amgb\nL::Int: Number of multigrid levels (passed to fem3d)\nk::Int: Polynomial order of elements (passed to fem3d)\np: Power parameter for the barrier (passed to amgb)\nverbose: Verbosity flag (passed to amgb)\nD: Operator structure matrix (passed to amgb, defaults to 3D operators)\nf: Source term function (passed to amgb, defaults to 3D source)\ng: Boundary condition function (passed to amgb, defaults to 3D BCs)\nOther arguments specific to fem3d or amgb\n\nReturns\n\nThe solution object from amgb.\n\nExample\n\nsol = fem3d_mpi_solve(Float64; L=2, k=3, p=1.0, verbose=true)\nprintln(\"Solution norm: \", norm(sol.z))\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierMPI.native_to_mpi","page":"API Reference","title":"MultiGridBarrierMPI.native_to_mpi","text":"native_to_mpi(g_native::Geometry{T, Matrix{T}, Vector{T}, SparseMatrixCSC{T,Int}, Discretization}) where {T, Discretization}\n\nCollective\n\nConvert a native Geometry object (with Julia arrays) to use MPI distributed types.\n\nThis is a collective operation. Each rank calls fem2d() to get the same native geometry, then this function converts:\n\nx::Matrix{T} -> x::MatrixMPI{T}\nw::Vector{T} -> w::VectorMPI{T}\noperators[key]::SparseMatrixCSC{T,Int} -> operators[key]::SparseMatrixMPI{T}\nsubspaces[key][i]::SparseMatrixCSC{T,Int} -> subspaces[key][i]::SparseMatrixMPI{T}\n\n\n\n\n\n","category":"function"},{"location":"api/#MultiGridBarrierMPI.mpi_to_native","page":"API Reference","title":"MultiGridBarrierMPI.mpi_to_native","text":"mpi_to_native(g_mpi::Geometry{T, MatrixMPI{T}, VectorMPI{T}, <:SparseMatrixMPI{T}, Discretization}) where {T, Discretization}\n\nCollective\n\nConvert an MPI Geometry object (with distributed MPI types) back to native Julia arrays.\n\nThis is a collective operation. This function converts:\n\nx::MatrixMPI{T} -> x::Matrix{T}\nw::VectorMPI{T} -> w::Vector{T}\noperators[key]::SparseMatrixMPI{T} -> operators[key]::SparseMatrixCSC{T,Int}\nsubspaces[key][i]::SparseMatrixMPI{T} -> subspaces[key][i]::SparseMatrixCSC{T,Int}\n\n\n\n\n\nmpi_to_native(sol_mpi::AMGBSOL{T, XType, WType, MType, Discretization}) where {T, XType, WType, MType, Discretization}\n\nCollective\n\nConvert an AMGBSOL solution object from MPI types back to native Julia types.\n\nThis is a collective operation that performs a deep conversion of the solution structure:\n\nz: MatrixMPI{T} -> Matrix{T} or VectorMPI{T} -> Vector{T}\nSOL_feasibility: NamedTuple with MPI types -> NamedTuple with native types\nSOL_main: NamedTuple with MPI types -> NamedTuple with native types\ngeometry: Geometry with MPI types -> Geometry with native types\n\n\n\n\n\nmpi_to_native(sol_mpi::ParabolicSOL{T, XType, WType, MType, Discretization}) where {T, XType, WType, MType, Discretization}\n\nCollective\n\nConvert a ParabolicSOL solution object from MPI types back to native Julia types.\n\nThis is a collective operation that performs a deep conversion of the parabolic solution:\n\ngeometry: Geometry with MPI types -> Geometry with native types\nts: Vector{T} (unchanged, already native)\nu: Vector{MatrixMPI{T}} -> Vector{Matrix{T}} (each time snapshot converted)\n\nExample\n\ng = fem2d_mpi(Float64; L=2)\nsol_mpi = parabolic_solve(g; h=0.5, p=1.0)\nsol_native = mpi_to_native(sol_mpi)\n\n\n\n\n\n","category":"function"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#Prerequisites","page":"Installation","title":"Prerequisites","text":"","category":"section"},{"location":"installation/#MPI","page":"Installation","title":"MPI","text":"MultiGridBarrierMPI.jl requires an MPI implementation. When you install the package, Julia automatically provides MPI.jl with MPI_jll (bundled MPI implementation).\n\nFor HPC environments, you may want to configure MPI.jl to use your system's MPI installation. See the MPI.jl documentation for details.","category":"section"},{"location":"installation/#MUMPS","page":"Installation","title":"MUMPS","text":"The package uses MUMPS for sparse direct solves through LinearAlgebraMPI.jl. MUMPS is typically available through your system's package manager or HPC module system.","category":"section"},{"location":"installation/#Package-Installation","page":"Installation","title":"Package Installation","text":"","category":"section"},{"location":"installation/#Basic-Installation","page":"Installation","title":"Basic Installation","text":"using Pkg\nPkg.add(url=\"https://github.com/sloisel/MultiGridBarrierMPI.jl\")","category":"section"},{"location":"installation/#Development-Installation","page":"Installation","title":"Development Installation","text":"To install the development version:\n\ngit clone https://github.com/sloisel/MultiGridBarrierMPI.jl\ncd MultiGridBarrierMPI.jl\njulia --project -e 'using Pkg; Pkg.instantiate()'","category":"section"},{"location":"installation/#Verification","page":"Installation","title":"Verification","text":"Test your installation with MPI:\n\ncd MultiGridBarrierMPI.jl\nmpiexec -n 2 julia --project test/runtests.jl\n\nAll tests should pass. Expected output:\n\nTest Summary:          | Pass  Total\nMultiGridBarrierMPI.jl |    2      2","category":"section"},{"location":"installation/#Initialization-Pattern","page":"Installation","title":"Initialization Pattern","text":"tip: Initialization Pattern\nInitialize MPI first, then load the package:\n\n# CORRECT\nusing MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\n# Now you can use the package\n\n# WRONG - MPI must be initialized before using MPI types\nusing MultiGridBarrierMPI\n# Missing MPI.Init() - will fail when calling functions","category":"section"},{"location":"installation/#Running-MPI-Programs","page":"Installation","title":"Running MPI Programs","text":"","category":"section"},{"location":"installation/#Multi-Rank-Execution","page":"Installation","title":"Multi-Rank Execution","text":"For distributed execution, create a script file (e.g., my_program.jl):\n\nusing MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\n\n# Your parallel code here\nsol = fem2d_mpi_solve(Float64; L=3, p=1.0)\nprintln(io0(), \"Solution computed!\")\n\nRun with MPI:\n\nmpiexec -n 4 julia --project my_program.jl\n\ntip: Output from Rank 0 Only\nUse io0() from LinearAlgebraMPI for output to avoid duplicate messages:println(io0(), \"This prints once from rank 0\")","category":"section"},{"location":"installation/#Troubleshooting","page":"Installation","title":"Troubleshooting","text":"","category":"section"},{"location":"installation/#MPI-Issues","page":"Installation","title":"MPI Issues","text":"If you see MPI-related errors, try rebuilding MPI.jl:\n\nusing Pkg; Pkg.build(\"MPI\")","category":"section"},{"location":"installation/#MUMPS-Issues","page":"Installation","title":"MUMPS Issues","text":"If MUMPS fails to load, ensure it's properly installed on your system and that LinearAlgebraMPI.jl can find it.","category":"section"},{"location":"installation/#Test-Failures","page":"Installation","title":"Test Failures","text":"If tests fail:\n\nEnsure you're using at least Julia 1.10 (LTS version)\nCheck all dependencies are installed: Pkg.status()\nRun with verbose output to see detailed errors","category":"section"},{"location":"installation/#Next-Steps","page":"Installation","title":"Next Steps","text":"Once installed, proceed to the User Guide to learn how to use the package.","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Markdown\nusing Pkg\nusing MultiGridBarrierMPI\nv = string(pkgversion(MultiGridBarrierMPI))\nmd\"# MultiGridBarrierMPI.jl $v\"\n\nA Julia package that bridges MultiGridBarrier.jl and LinearAlgebraMPI.jl for distributed multigrid barrier computations.","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"MultiGridBarrierMPI.jl extends the MultiGridBarrier.jl package to work with LinearAlgebraMPI.jl's distributed matrix and vector types. This enables efficient parallel computation of multigrid barrier methods across multiple MPI ranks using pure Julia distributed types (no PETSc required).","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"1D, 2D, and 3D Support: Full support for 1D elements, 2D triangular, and 3D hexahedral finite elements\nSeamless Integration: Drop-in replacement for MultiGridBarrier's native types\nPure Julia MPI: Uses LinearAlgebraMPI.jl for distributed linear algebra\nType Conversion: Easy conversion between native Julia arrays and MPI distributed types\nMPI-Aware: All operations correctly handle MPI collective requirements\nMUMPS Solver: Uses MUMPS direct solver for accurate Newton iterations","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"Solve a 2D p-Laplace problem with distributed MPI types. Save this code to example.jl:\n\nusing MPI\nMPI.Init()\n\nusing MultiGridBarrierMPI\nusing LinearAlgebraMPI\nusing MultiGridBarrier\n\n# Solve with MPI distributed types (L=3 refinement levels)\nsol_mpi = fem2d_mpi_solve(Float64; L=3, p=1.0, verbose=false)\n\n# Convert to native types for visualization\nsol_native = mpi_to_native(sol_mpi)\n\n# Only rank 0 creates the plot\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\nif rank == 0\n    using PyPlot\n    plot(sol_native)\n    savefig(\"solution.png\")\n    println(\"Plot saved to solution.png\")\nend\n\nRun with MPI:\n\nmpiexec -n 4 julia --project example.jl","category":"section"},{"location":"#Documentation-Contents","page":"Home","title":"Documentation Contents","text":"Pages = [\"installation.md\", \"guide.md\", \"api.md\"]\nDepth = 2","category":"section"},{"location":"#Package-Ecosystem","page":"Home","title":"Package Ecosystem","text":"This package is part of a larger ecosystem:\n\nMultiGridBarrier.jl: Core multigrid barrier method implementation (1D, 2D, and 3D)\nLinearAlgebraMPI.jl: Pure Julia distributed linear algebra with MPI\nMPI.jl: Julia MPI bindings for distributed computing","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.10 or later (LTS version)\nMPI installation (OpenMPI, MPICH, or Intel MPI)\nMUMPS for sparse direct solves\nAt least 2 MPI ranks recommended for testing","category":"section"},{"location":"#Citation","page":"Home","title":"Citation","text":"If you use this package in your research, please cite:\n\n@software{multigridbarriermpi,\n  author = {Loisel, Sebastien},\n  title = {MultiGridBarrierMPI.jl: Distributed Multigrid Barrier Methods with MPI},\n  year = {2024},\n  url = {https://github.com/sloisel/MultiGridBarrierMPI.jl}\n}","category":"section"},{"location":"#License","page":"Home","title":"License","text":"This package is licensed under the MIT License.","category":"section"}]
}
